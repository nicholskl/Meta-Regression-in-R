[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Implementing Meta-Regression with R",
    "section": "",
    "text": "Meta-analysis is a statistical approach used to synthesize results from multiple studies to derive conclusions with greater statistical power and generalizability. By combining data from several independent studies, meta-analysis helps to resolve uncertainty when individual studies disagree and provides a more precise estimate of the effect size.\n\n\nMeta-analysis involves systematically reviewing, extracting, and statistically combining data from a collection of studies that address a common research question. This technique enables researchers to:\n\nSummarize Evidence: Meta-analysis aggregates findings, offering a comprehensive overview of what the research community knows about a particular topic.\nIncrease Power: By pooling data from multiple studies, meta-analysis enhances the statistical power to detect effects that individual studies might miss.\nAddress Variability: It helps to understand the variation in study results, identifying factors that contribute to differences across studies.\n\n\n\n\nMeta-analysis goes beyond simple summary by:\n\nEstimating Overall Effect: It calculates a weighted average effect size, giving more weight to studies with larger samples or more precise estimates.\nExploring Heterogeneity: It assesses the degree of variability among study results to understand if differences are due to chance or other underlying factors.\nIdentifying Subgroup Effects: By stratifying data, meta-analysis can explore how effects vary across different subgroups, such as age, gender, or intervention type.\n\n\n\n\nWhile meta-analysis is a valuable tool, it has a few challenges that can make a traditional and straightforward approach difficult. Two of the most significant problems are heterogeneity in methods and dependence among study outcomes.\n\nHeterogeneity in Methods: Studies included in a meta-analysis often differ in their design, populations, interventions, and outcome measures. This heterogeneity can lead to inconsistent findings and complicate the interpretation of the combined results. Identifying and accounting for these differences is crucial for a robust meta-analysis. This is particularly salient in psychology studies where papers even on a well traditioned and niche idea can vary wildly in methodology (see this famous and controversial meta-analysis on nudges for an example.\nDependence Among Study Outcomes: When studies use similar methodologies or populations, their outcomes may not be entirely independent. This dependence can bias the results and undermine the validity of the meta-analysis. Properly addressing dependence is essential to avoid overestimating the precision of the combined effect size. This can be true when several effect sizes might be produced from a single paper, when several effect sizes might need to be clustered by the measure that they use, or by the types of participants included, for example\n\n\n\n\nAddressing the issues of heterogeneity and dependence is critical for a meaningful meta-analysis. One advanced technique to tackle these problems is meta-regression. Meta-regression extends traditional meta-analysis by incorporating study-level covariates, moderators, and allows for mixed-level modeling, allowing researchers to:\n\nModel Heterogeneity: By including variables that might explain differences between studies, meta-regression helps to account for and understand sources of heterogeneity.\nAdjust for Dependence: It provides a framework to adjust for potential correlations among study outcomes, ensuring more accurate and reliable results.\n\nIn the next section, I will describe meta-regression in greater detail, exploring how it works and how it can be applied to enhance the insights gained from meta-analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Meta-Regression in R",
    "section": "",
    "text": "Implementing Meta-Regression with R\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nKristopher Nichols\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html#what-is-meta-regression",
    "href": "posts/post-with-code/index.html#what-is-meta-regression",
    "title": "Implementing Meta-Regression with R",
    "section": "What is Meta-Regression?",
    "text": "What is Meta-Regression?\nMeta-regression is a statistical tool used in meta-analysis, where multiple studies are combined to understand a phenomenon more comprehensively. This technique is particularly valuable when studies vary significantly in terms of settings, methodologies, or populations. Meta-regression explores the relationship between study characteristics (like sample size or study quality) and study outcomes, helping to explain the heterogeneity (variability) among the results of these studies. Here, we‚Äôll dive into the basics of meta-regression using the dmetar and metafor packages and the MVRegressionData dataset in R.\n\nMeta-regression is an extension of the standard meta-analysis, allowing researchers to investigate how study-level covariates impact the effect size (the quantitative measure of the magnitude of the experimental effect). For instance, in clinical research, larger studies might show different effect sizes compared to smaller studies due to systematic differences in patient selection or treatment implementation. In psychology studies, where methodological variance between studies can be substantial, it can be difficult to collapse across several studies as the differences in design may affect the effect sizes and drive unexplained variance."
  },
  {
    "objectID": "posts/post-with-code/index.html#how-does-meta-regression-solve-this-problem",
    "href": "posts/post-with-code/index.html#how-does-meta-regression-solve-this-problem",
    "title": "Implementing Meta-Regression with R",
    "section": "How Does Meta-Regression Solve This Problem?",
    "text": "How Does Meta-Regression Solve This Problem?\nThe primary goal of meta-regression is to determine whether covariates significantly contribute to the observed heterogeneity among the results. This not only helps in understanding the factors influencing the outcomes but also in improving the design of future studies and tailoring interventions more effectively.\nMeta-regression analyzes the relationship between effect sizes and study-level characteristics by fitting a regression model. This allows researchers to adjust for different variables and isolate the effects of specific characteristics on the study outcome. By doing so, meta-regression provides a more nuanced understanding of the factors that contribute to the differences in study results. Moreover, a traditional meta-analysis does not account for dependency across effect sizes as nimbly as meta-regression does, with meta-regression offering multiple avenues for correcting dependency (mixed-effects models, clustering standard errors, cluster wild bootstrapping)."
  },
  {
    "objectID": "posts/post-with-code/index.html#practical-example-using-dmetar-and-mvregressiondata",
    "href": "posts/post-with-code/index.html#practical-example-using-dmetar-and-mvregressiondata",
    "title": "Implementing Meta-Regression with R",
    "section": "Practical Example Using dmetar and MVRegressionData",
    "text": "Practical Example Using dmetar and MVRegressionData\nTo illustrate, let‚Äôs use the dmetar package in R, which is designed for conducting robust meta-analyses, including meta-regression. The MVRegressionData dataset included in this package will serve as our example data.\n\nSetting Up the Environment\nFirst, you‚Äôll need to install and load the dmetar package. If you haven‚Äôt installed it yet, you can do so using the following commands in R:\n\n#install.packages(\"dmetar\")\nlibrary(dmetar)\nlibrary(metafor)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\ndata(\"MVRegressionData\", package = \"dmetar\")\ndata(\"dat.bcg\", package = \"metafor\")\n\nThis dataset includes multiple variables, but we will focus on the effect sizes and a few key covariates for our meta-regression analysis.\nFirst we take a look at the data where in addition to the trial number, author(s), and publication year, the dataset contains details about the number of treated (vaccinated) subjects who tested positive and negative for tuberculosis (tpos and tneg, respectively), as well as the number of control (non-vaccinated) subjects who were tuberculosis positive and negative (cpos and cneg, respectively). Furthermore, the dataset includes the absolute latitude of the study location (in degrees) and the treatment allocation method used (random, alternate, or systematic assignment) for each trial.\n\nkable(dat.bcg, row.names = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrial\nauthor\nyear\ntpos\ntneg\ncpos\ncneg\nablat\nalloc\n\n\n\n\n1\nAronson\n1948\n4\n119\n11\n128\n44\nrandom\n\n\n2\nFerguson & Simes\n1949\n6\n300\n29\n274\n55\nrandom\n\n\n3\nRosenthal et al\n1960\n3\n228\n11\n209\n42\nrandom\n\n\n4\nHart & Sutherland\n1977\n62\n13536\n248\n12619\n52\nrandom\n\n\n5\nFrimodt-Moller et al\n1973\n33\n5036\n47\n5761\n13\nalternate\n\n\n6\nStein & Aronson\n1953\n180\n1361\n372\n1079\n44\nalternate\n\n\n7\nVandiviere et al\n1973\n8\n2537\n10\n619\n19\nrandom\n\n\n8\nTPT Madras\n1980\n505\n87886\n499\n87892\n13\nrandom\n\n\n9\nCoetzee & Berjak\n1968\n29\n7470\n45\n7232\n27\nrandom\n\n\n10\nRosenthal et al\n1961\n17\n1699\n65\n1600\n42\nsystematic\n\n\n11\nComstock et al\n1974\n186\n50448\n141\n27197\n18\nsystematic\n\n\n12\nComstock & Webster\n1969\n5\n2493\n3\n2338\n33\nsystematic\n\n\n13\nComstock et al\n1976\n27\n16886\n29\n17825\n33\nsystematic\n\n\n\n\n\nFor these particular data the main outcome is binary, positive or negative, so we will be dealing with log of relative risk. The first step in meta-regression (and meta-analysis, generally) is obtaining effect size and variance statistics \\(y_i\\) and \\(v_i\\). These are the primary units of analysis for most functions in metafor, and can be handled easily via the escalc function from metafor.\n\ndat &lt;- escalc(measure = \"RR\", ai = tpos, bi = tneg, ci = cpos, \n              di = cneg, data = dat.bcg, append = TRUE)\n\nkable(dat[,-c(4:7)], row.names = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrial\nauthor\nyear\nablat\nalloc\nyi\nvi\n\n\n\n\n1\nAronson\n1948\n44\nrandom\n-0.8893113\n0.3255848\n\n\n2\nFerguson & Simes\n1949\n55\nrandom\n-1.5853887\n0.1945811\n\n\n3\nRosenthal et al\n1960\n42\nrandom\n-1.3480731\n0.4153680\n\n\n4\nHart & Sutherland\n1977\n52\nrandom\n-1.4415512\n0.0200100\n\n\n5\nFrimodt-Moller et al\n1973\n13\nalternate\n-0.2175473\n0.0512102\n\n\n6\nStein & Aronson\n1953\n44\nalternate\n-0.7861156\n0.0069056\n\n\n7\nVandiviere et al\n1973\n19\nrandom\n-1.6208982\n0.2230172\n\n\n8\nTPT Madras\n1980\n13\nrandom\n0.0119523\n0.0039616\n\n\n9\nCoetzee & Berjak\n1968\n27\nrandom\n-0.4694176\n0.0564342\n\n\n10\nRosenthal et al\n1961\n42\nsystematic\n-1.3713448\n0.0730248\n\n\n11\nComstock et al\n1974\n18\nsystematic\n-0.3393588\n0.0124122\n\n\n12\nComstock & Webster\n1969\n33\nsystematic\n0.4459134\n0.5325058\n\n\n13\nComstock et al\n1976\n33\nsystematic\n-0.0173139\n0.0714047\n\n\n\n\n\n\n\nConducting Meta-Regression\nNow, let‚Äôs perform a meta-regression. We‚Äôll use the rma function from the metafor package (which dmetar depends on) to regress the effect sizes on some of the covariates included in our dataset:\n\n# Running the meta-regression model\nmeta_reg &lt;- rma(yi = yi, sei = sei, data = MVRegressionData)\nsummary(meta_reg)\n\n\nRandom-Effects Model (k = 36; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n-17.6759   35.3517   39.3517   42.4624   39.7267   \n\ntau^2 (estimated amount of total heterogeneity): 0.0771 (SE = 0.0306)\ntau (square root of estimated tau^2 value):      0.2776\nI^2 (total heterogeneity / total variability):   63.66%\nH^2 (total variability / sampling variability):  2.75\n\nTest for Heterogeneity:\nQ(df = 35) = 91.8594, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.5738  0.0602  9.5265  &lt;.0001  0.4557  0.6918  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the code above:\n\nyi represents the effect sizes.\nsei stands for the standard errors of the effect sizes."
  },
  {
    "objectID": "posts/post-with-code/index.html#basic-concept-of-meta-regression",
    "href": "posts/post-with-code/index.html#basic-concept-of-meta-regression",
    "title": "Implementing Meta-Regression with R",
    "section": "Basic Concept of Meta-Regression",
    "text": "Basic Concept of Meta-Regression\nMeta-regression models the effect size from each study as a function of study-level characteristics (covariates). It essentially extends the random-effects model in meta-analysis by incorporating these covariates to explain variations in the effect sizes."
  },
  {
    "objectID": "posts/post-with-code/index.html#the-meta-regression-model",
    "href": "posts/post-with-code/index.html#the-meta-regression-model",
    "title": "Implementing Meta-Regression with R",
    "section": "The Meta-Regression Model",
    "text": "The Meta-Regression Model\nThe meta-regression model can be represented as:\n\\[\n\\theta_k = \\theta + \\beta x_k + \\epsilon_k + \\zeta_k\n\\]\nWhere, \\(\\theta_k\\) is the observed effect size in the \\(k\\)-th study. \\(\\theta\\) is the overall average effect size across all studies, serving as the intercept in the meta-regression model. This represents the expected effect size when all covariates are zero. \\(\\beta\\) is the regression coefficient that quantifies the relationship between the study-level covariate \\(x_k\\) and the effect size \\(\\theta_k\\). It indicates how much the effect size changes for a one-unit change in the covariate. \\(x_k\\) is the \\(k\\)-th study-level covariate. This could be any characteristic of the study, such as sample size, study design, or population characteristics, that might influence the effect size.\n\\(\\epsilon_k\\) is the within-study error term, capturing the sampling variability or random error in the effect size estimate for the \\(k\\)-th study. This term accounts for the fact that even studies with the same covariate value may have different observed effect sizes due to random sampling variation. \\(\\zeta_k\\) is the between-study error term, representing the residual heterogeneity not explained by the covariates. This term captures the variability in effect sizes that is attributable to unmeasured or unknown factors.\nMeta-regression supports both categorical and continuous predictors, can be setup in a way that researchers familiar with regression should be used to, and also supports mixed-effect modeling, allowing for random intercept of study data (e.g., study ID) to model dependency in effect sizes."
  },
  {
    "objectID": "posts/post-with-code/index.html#steps-in-conducting-meta-regression",
    "href": "posts/post-with-code/index.html#steps-in-conducting-meta-regression",
    "title": "Implementing Meta-Regression with R",
    "section": "Steps in Conducting Meta-Regression",
    "text": "Steps in Conducting Meta-Regression\n\nData Collection and Coding:\n\nCollect effect sizes (e.g., mean differences, odds ratios) from each study.\nGather study-level covariates that might explain heterogeneity (e.g., sample size, study design, population characteristics).\n\nModel Specification:\n\nSpecify the meta-regression model, including the intercept and covariates of interest.\nChoose between fixed-effects and random-effects models. The random-effects model is more common as it accounts for both within-study and between-study variability.\n\nEstimation:\n\nEstimate the model parameters \\(\\beta_j\\) using weighted least squares or maximum likelihood methods. The weights are typically the inverse of the variance of the effect size estimates, giving more weight to more precise studies.\n\nInterpretation:\n\nInterpret the coefficients \\(\\beta_j\\). A significant \\(\\beta_j\\) indicates that the corresponding covariate \\(X{ij}\\) explains some of the variability in effect sizes.\nAssess the residual heterogeneity \\(u_i\\) to see how much variation remains unexplained."
  },
  {
    "objectID": "posts/post-with-code/index.html#key-statistical-considerations",
    "href": "posts/post-with-code/index.html#key-statistical-considerations",
    "title": "Implementing Meta-Regression with R",
    "section": "Key Statistical Considerations",
    "text": "Key Statistical Considerations\n\nHeterogeneity Assessment:\n\nQuantify the residual heterogeneity after accounting for covariates using \\(\\tau^2\\), the between-study variance.\nUse \\(I^2\\) statistics to describe the proportion of total variability due to heterogeneity rather than sampling error.\n\nModel Fit and Diagnostics:\n\nEvaluate the fit of the meta-regression model using goodness-of-fit statistics.\nCheck for potential multicollinearity among covariates, which can inflate the variance of coefficient estimates.\nConduct sensitivity analyses to assess the robustness of the results.\n\nDependence Adjustment:\n\nIf study outcomes are correlated, adjust for this dependence within the meta-regression framework to avoid biased estimates.\nUse cluster-robust standard errors or multilevel models if there are multiple effect sizes from the same study."
  },
  {
    "objectID": "posts/post-with-code/index.html#advantages-of-meta-regression",
    "href": "posts/post-with-code/index.html#advantages-of-meta-regression",
    "title": "Implementing Meta-Regression with R",
    "section": "Advantages of Meta-Regression",
    "text": "Advantages of Meta-Regression\n\nEnhanced Understanding: By incorporating covariates, meta-regression helps to identify and quantify factors contributing to heterogeneity.\nImproved Accuracy: Adjusting for study-level characteristics and dependencies leads to more accurate and reliable estimates of the overall effect size.\nFlexibility: Meta-regression can accommodate continuous, categorical, and interaction terms, providing a flexible approach to exploring complex relationships. Moreover, a traditional meta-analysis does not account for dependency across effect sizes as nimbly as meta-regression does, with meta-regression offering multiple avenues for correcting dependency (mixed-effects models, clustering standard errors, cluster wild bootstrapping)."
  },
  {
    "objectID": "posts/post-with-code/index.html#practical-example-using-dmetar-and-dat.bcg",
    "href": "posts/post-with-code/index.html#practical-example-using-dmetar-and-dat.bcg",
    "title": "Implementing Meta-Regression with R",
    "section": "Practical Example Using dmetar and dat.bcg",
    "text": "Practical Example Using dmetar and dat.bcg\nTo illustrate, let‚Äôs use the metafor and dmetar package in R, which is designed for conducting robust meta-analyses, including meta-regression. The datasets included in these packages will serve as our example data.\n\nSetting Up the Environment\nFirst, you‚Äôll need to install and load the packages. If you haven‚Äôt installed it yet, you can do so using the following commands in R:\n\n#install.packages(\"dmetar\")\nlibrary(dmetar)\nlibrary(metafor)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\ndata(\"MVRegressionData\", package = \"dmetar\")\ndata(\"dat.bcg\", package = \"metafor\")\n\nThis dataset includes multiple variables, but we will focus on the effect sizes and a few key covariates for our meta-regression analysis.\nFirst we take a look at the data where in addition to the trial number, author(s), and publication year, the dataset contains details about the number of treated (vaccinated) subjects who tested positive and negative for tuberculosis (tpos and tneg, respectively), as well as the number of control (non-vaccinated) subjects who were tuberculosis positive and negative (cpos and cneg, respectively). Furthermore, the dataset includes the absolute latitude of the study location (in degrees) and the treatment allocation method used (random, alternate, or systematic assignment) for each trial.\n\nkable(dat.bcg, row.names = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrial\nauthor\nyear\ntpos\ntneg\ncpos\ncneg\nablat\nalloc\n\n\n\n\n1\nAronson\n1948\n4\n119\n11\n128\n44\nrandom\n\n\n2\nFerguson & Simes\n1949\n6\n300\n29\n274\n55\nrandom\n\n\n3\nRosenthal et al\n1960\n3\n228\n11\n209\n42\nrandom\n\n\n4\nHart & Sutherland\n1977\n62\n13536\n248\n12619\n52\nrandom\n\n\n5\nFrimodt-Moller et al\n1973\n33\n5036\n47\n5761\n13\nalternate\n\n\n6\nStein & Aronson\n1953\n180\n1361\n372\n1079\n44\nalternate\n\n\n7\nVandiviere et al\n1973\n8\n2537\n10\n619\n19\nrandom\n\n\n8\nTPT Madras\n1980\n505\n87886\n499\n87892\n13\nrandom\n\n\n9\nCoetzee & Berjak\n1968\n29\n7470\n45\n7232\n27\nrandom\n\n\n10\nRosenthal et al\n1961\n17\n1699\n65\n1600\n42\nsystematic\n\n\n11\nComstock et al\n1974\n186\n50448\n141\n27197\n18\nsystematic\n\n\n12\nComstock & Webster\n1969\n5\n2493\n3\n2338\n33\nsystematic\n\n\n13\nComstock et al\n1976\n27\n16886\n29\n17825\n33\nsystematic\n\n\n\n\n\nFor these particular data the main outcome is binary, positive or negative, so we will be dealing with log of relative risk. The first step in meta-regression (and meta-analysis, generally) is obtaining effect size and variance statistics \\(y_i\\) and \\(v_i\\). These are the primary units of analysis for most functions in metafor, and can be handled easily via the escalc function from metafor.\n\ndata_es &lt;- escalc(measure = \"RR\", ai = tpos, bi = tneg, ci = cpos, \n              di = cneg, data = dat.bcg, append = TRUE)\n\nkable(data_es[,-c(4:7)], row.names = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrial\nauthor\nyear\nablat\nalloc\nyi\nvi\n\n\n\n\n1\nAronson\n1948\n44\nrandom\n-0.8893113\n0.3255848\n\n\n2\nFerguson & Simes\n1949\n55\nrandom\n-1.5853887\n0.1945811\n\n\n3\nRosenthal et al\n1960\n42\nrandom\n-1.3480731\n0.4153680\n\n\n4\nHart & Sutherland\n1977\n52\nrandom\n-1.4415512\n0.0200100\n\n\n5\nFrimodt-Moller et al\n1973\n13\nalternate\n-0.2175473\n0.0512102\n\n\n6\nStein & Aronson\n1953\n44\nalternate\n-0.7861156\n0.0069056\n\n\n7\nVandiviere et al\n1973\n19\nrandom\n-1.6208982\n0.2230172\n\n\n8\nTPT Madras\n1980\n13\nrandom\n0.0119523\n0.0039616\n\n\n9\nCoetzee & Berjak\n1968\n27\nrandom\n-0.4694176\n0.0564342\n\n\n10\nRosenthal et al\n1961\n42\nsystematic\n-1.3713448\n0.0730248\n\n\n11\nComstock et al\n1974\n18\nsystematic\n-0.3393588\n0.0124122\n\n\n12\nComstock & Webster\n1969\n33\nsystematic\n0.4459134\n0.5325058\n\n\n13\nComstock et al\n1976\n33\nsystematic\n-0.0173139\n0.0714047\n\n\n\n\n\n\n\nConducting Meta-Regression\nFor performing regression, I use the rma class of functions from the metafor package to regress the effect sizes on some of the covariates included in our dataset. The rma function is the workhorse of the metafor package for meta-regression and has a few flexible variants, including rma.mv for mixed-effect/multivariate linear modeling. The normal random-effects regression model can be fit with rma(yi, vi, data = dat).\nMany R users will be familiar with the formula syntax used in functions like lm() and glm(). Similarly, one can specify the desired meta-analytic model using the mods argument, which accepts a formula in the form ~ model (e.g., mods = ~ mod1 + mod2 + mod3). This approach allows for the easy addition of interactions, polynomial terms, and factors to the model. A fixed-effects model can be fitted with rma(yi, vi, data = dat, method = \"FE\").\n\n# Running the meta-regression model\nmodel_1 &lt;- rma(yi, vi, data = data_es)\nsummary(model_1)\n\n\nRandom-Effects Model (k = 13; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n-12.2024   24.4047   28.4047   29.3746   29.7381   \n\ntau^2 (estimated amount of total heterogeneity): 0.3132 (SE = 0.1664)\ntau (square root of estimated tau^2 value):      0.5597\nI^2 (total heterogeneity / total variability):   92.22%\nH^2 (total variability / sampling variability):  12.86\n\nTest for Heterogeneity:\nQ(df = 12) = 152.2330, p-val &lt; .0001\n\nModel Results:\n\nestimate      se     zval    pval    ci.lb    ci.ub      \n -0.7145  0.1798  -3.9744  &lt;.0001  -1.0669  -0.3622  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe estimated between-study variance (ùúè2œÑ2) was 0.3132 (SE = 0.1664), with the square root of this variance (ùúèœÑ) being 0.5597. This high ùúèœÑ value indicates considerable variability in effect sizes across studies. The \\(I^2\\) statistic, which represents the percentage of total variation due to heterogeneity rather than chance, was 92.22% which is a fairly high value. Here‚Äôs a general rule of thumb for interpreting these values where anything over .75 (75%) could be considered as considerable heterogeneity between studies. This substantial heterogeneity suggests that differences between studies contribute significantly to the variability in effect sizes. Furthermore, the \\(H^2\\) statistic, which compares total variability to sampling variability, was 12.86, indicating that the observed variability greatly exceeds what would be expected by sampling error alone.\nThe test for heterogeneity was significant (ùëÑ(12)=152.23Q(12)=152.23, ùëù&lt;.0001), confirming the presence of substantial heterogeneity among the studies. This suggests that the effect sizes are not consistent across studies and that additional factors may influence the variability.\nThe model results showed an overall effect size estimate (log relative risk) of -0.7145 (SE = 0.1798). Transforming these values with the exp() function, we get a value of .49 with a 95% CI of (0.34, 0.70). The negative estimate indicates that the vaccinated group had a lower risk of tuberculosis infection compared to the control group. Moreover, for the vaccinated subjects there was nearly half the risk of getting tuberculosis. This test was highly significant (ùëß=‚àí3.97z=‚àí3.97, ùëù&lt;.0001p&lt;.0001), with a 95% confidence interval ranging from -1.0669 to -0.3622. The confidence interval not including zero further supports the conclusion that the vaccination significantly reduces the risk of tuberculosis.\nThese model fit results can be obtained with confint(). A key vizualization of effect sizes in a meta-analysis paper is the forest plot, and this can also be handled via metafor with the forest() function.\n\nkable(confint(model_1))\n\n\n\n\n\nestimate\nci.lb\nci.ub\n\n\n\n\ntau^2\n0.3132433\n0.1196953\n1.111486\n\n\ntau\n0.5596815\n0.3459701\n1.054271\n\n\nI^2(%)\n92.2213861\n81.9177227\n97.678090\n\n\nH^2\n12.8557608\n5.5302769\n43.067984\n\n\n\n\nforest(model_1)\n\n\n\n\n\n\n\n\nIn this forest plot, mostly what we are looking for is at the bottom most diamond which represents the overall model for the meta-regression, as well as how many of the CIs include 0. These plots are highly cutomizable and can be built upon like below, allowing for researchers to also graph additional variables with information about the design.\n\nforest(model_1,\n       xlab = \"Effect Size (Log Relative Risk)\", # Label for x-axis\n       slab = paste(dat.bcg$author, dat.bcg$year, sep=\", \"), # Study labels\n       ilab = dat.bcg$alloc, # Additional column for treatment allocation method\n       ilab.xpos = -4, # Position of the additional column\n       xlim = c(-10, 5), # X-axis limits\n       cex = 0.75, # Text size\n       alim = c(-3, 2), # Limits for the x-axis (effect sizes)\n       at = log(c(0.1, 0.25, 0.5, 1, 2, 4, 8)), # Custom ticks on x-axis\n       col = \"blue\", # Color for the summary polygon\n       border = \"darkblue\", # Border color for the summary polygon\n       psize = 1.5, # Size of the points\n       mlab = \"Random Effects Model\", # Label for the summary effect\n       order = order(dat.bcg$year) # Order studies by year\n)\n\n\n\n\n\n\n\n\nAdditionally, the package dmetar offers a great deal of support for meta-regression beyond the metafor package and accepts rma objects for many of its functions. For example, a crucial aspect of meta-analysis (and meta-regression) is outlier analysis. This is typically done in advance of analysis via a Cook‚Äôs test, but can also be done via the find.outliers() package.\n\nfind.outliers(model_1)\n\nIdentified outliers (REML) \n------------------------- \n\"4\", \"8\" \n \nResults with outliers removed \n----------------------------- \n\nRandom-Effects Model (k = 11; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.2281 (SE = 0.1501)\ntau (square root of estimated tau^2 value):      0.4776\nI^2 (total heterogeneity / total variability):   83.01%\nH^2 (total variability / sampling variability):  5.89\n\nTest for Heterogeneity:\nQ(df = 10) = 39.6122, p-val &lt; .0001\n\nModel Results:\n\nestimate      se     zval    pval    ci.lb    ci.ub      \n -0.7082  0.1784  -3.9701  &lt;.0001  -1.0578  -0.3586  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, the results show us what the outliers were, as well as the results with those outliers removed. here we can see that the \\(I^2\\) was substantially lowered by removing those outliers. We can then use this code to fit into forest again and build a plot of the new results.\n\nforest(find.outliers(model_1))\n\n\n\n\n\n\n\n\n\n\nMixed Effect Models\nWhile fitting a random-effects model allows researchers to partially handle dependence in effects from any given paper, part of heterogeneity in analysis could be due to moderators. rma also allows researchers to fit a mixed-effect model relatively easy.\n\nmef_1 &lt;- rma(yi, vi, mods = ~ ablat + year, data = data_es)\nsummary(mef_1)\n\n\nMixed-Effects Model (k = 13; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n -8.1069   16.2137   24.2137   25.4241   32.2137   \n\ntau^2 (estimated amount of residual heterogeneity):     0.1108 (SE = 0.0845)\ntau (square root of estimated tau^2 value):             0.3328\nI^2 (residual heterogeneity / unaccounted variability): 71.98%\nH^2 (unaccounted variability / sampling variability):   3.57\nR^2 (amount of heterogeneity accounted for):            64.63%\n\nTest for Residual Heterogeneity:\nQE(df = 10) = 28.3251, p-val = 0.0016\n\nTest of Moderators (coefficients 2:3):\nQM(df = 2) = 12.2043, p-val = 0.0022\n\nModel Results:\n\n         estimate       se     zval    pval     ci.lb    ci.ub     \nintrcpt   -3.5455  29.0959  -0.1219  0.9030  -60.5724  53.4814     \nablat     -0.0280   0.0102  -2.7371  0.0062   -0.0481  -0.0080  ** \nyear       0.0019   0.0147   0.1299  0.8966   -0.0269   0.0307     \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe estimated residual heterogeneity is ùúè^2=0.1108œÑ^2=0.1108, indicating that incorporating the two moderators in the model accounts for 65% of the total heterogeneity ((0.3132‚àí0.1108)/0.3132(0.3132‚àí0.1108)/0.3132). The omnibus test shows that we can reject the null hypothesis ùêª0:ùõΩ1=ùõΩ2=0H0‚Äã:Œ≤1‚Äã=Œ≤2‚Äã=0 (QM = 12.20, df = 2, p &lt; 0.01), implying that the moderators collectively have a significant effect. However, only absolute latitude significantly influences vaccine effectiveness (for ùêª0:ùõΩ1=0H0‚Äã:Œ≤1‚Äã=0, z = -2.74, p &lt; 0.01), while the other moderator does not (for ùêª0:ùõΩ2=0H0‚Äã:Œ≤2‚Äã=0, z = 0.13, p = 0.90). The significant test for residual heterogeneity (QE = 28.33, df = 10, p &lt; 0.01) suggests that there may be other unconsidered moderators affecting vaccine effectiveness.\nWe can then compare these two models in a number of ways, starting first with an anova test, comparing the models similarly to how one would compare two nested regression models.\n\nanova(mef_1, model_1)\n\nWarning: REML comparisons not meaningful for models with different fixed effects\n(use 'refit=TRUE' to refit both models based on ML estimation).\n\n\n\n        df     AIC     BIC    AICc   logLik    LRT   pval       QE  tau^2 \nFull     4 24.2137 25.4241 32.2137  -8.1069                28.3251 0.1108 \nReduced  2 28.4047 29.3746 29.7381 -12.2024 8.1910 0.0166 152.2330 0.3132 \n             R^2 \nFull             \nReduced 64.6322% \n\n\nHere, we see the the full model was a better fit (as indicated by a lower AIC) than the reduced model. We can also visualize the model comparison via funnel plots. The funnel() function generates funnel plots, which are useful for detecting heterogeneity and certain types of publication bias (Rothstein et al.¬†2005). For models without moderators, the plot displays the observed outcomes on the horizontal axis and their corresponding standard errors (i.e., the square root of the sampling variances) on the vertical axis. A vertical line represents the model estimate, with a pseudo confidence interval region around this estimate, bounded by ¬±1.96 * SE, where SE is the standard error from the vertical axis. For models with moderators, the plot shows residuals on the horizontal axis against their corresponding standard errors, with a vertical line at zero and a pseudo confidence interval region of ¬±1.96 * SE.\n\nfunnel(model_1, main = \"Random-Effects Model\")\n\n\n\n\n\n\n\nfunnel(mef_1, main = \"Mixed-Effects Model\")\n\n\n\n\n\n\n\n\n\n\nMultilevel Meta-Analysis\nMeta-analysis has multiple levels due to the way data is structured. In the random-effects model, two sources of variability are accounted for: the sampling error \\(\\epsilon_k\\)of individual studies and the between-study heterogeneity \\(\\zeta_k\\). The goal is to estimate the mean Œº of the distribution of true effect sizes.\nThese two error terms correspond to two levels in meta-analysis data: the participant level (level 1) and the study level (level 2). At the participant level, data often reaches us in pooled form (e.g., means and standard deviations). Pooling at the study level is performed as part of the meta-analysis. Participants are nested within studies, creating a multilevel structure implicitly described by the random-effects model.\n\n\nExtending to a Three-Level Model\n\\(\\hat{\\theta}{ij} = \\mu + \\zeta^{(2)}{ij} + \\zeta^{(3)}{j} + \\epsilon{ij}\\)\nStatistical independence is crucial when pooling effect sizes in meta-analysis. Dependency between effect sizes can reduce heterogeneity and lead to false-positive results. Dependencies can arise from study authors (e.g., multiple sites or interventions within a study) or the meta-analyst (e.g., studies from different cultural regions).\nTo account for such dependencies, we can introduce a third level in the meta-analysis model. This level can capture nested structures within studies or among clusters of studies. A three-level model explicitly incorporates these dependencies, improving the accuracy of our meta-analytic estimates.\nA three-level model involves three pooling steps: aggregating participant data within studies, nesting effect sizes within clusters, and pooling cluster effects to estimate the overall true effect size (Œº). This model explicitly accounts for within-cluster and between-cluster heterogeneity, represented by two heterogeneity terms: \\(\\zeta^{(2)}_{ij}\\) (within-cluster) and \\(\\zeta^{(3)}_{j}\\)‚Äã (between-cluster).\n\n\nMultilevel Models in R\nThese models are particularly well suited to being carried out in the metafor package as well, though using a different function - the rma.mv() function. Below we will make use of the Chernobyl dataset from dmetar. Below we specify a model that is largely similar to our models from earlier but with different clustering.\nVia the random argument, we specify a formula which defines the (nested) random effects. For a three-level model, the formula always starts with ~ 1, followed by a vertical bar |. Behind the vertical bar, we assign a random effect to a grouping variable (such as studies, measures, regions, etc.). This random intercept is the same conceptually to those implemented in mixed-effects models researchers may be more accustomed to.\nThere is a specific syntax in which we can instruct rma.mv to assume nested random effects. This is achieved by using a slash (/) to separate the higher- and lower-level grouping variables. On the left side of the slash, we place the level 3 (cluster) variable, and on the right side, we insert the lower-level variable nested within the larger cluster. The general structure of the formula is: ~ 1 | cluster/effects_within_cluster.\nFor our example, we assume that individual effect sizes (level 2, defined by es.id) are nested within studies (level 3, defined by author). This results in the following formula: ~ 1 | author/es.id.\n\nlibrary(esc)\ndata(\"Chernobyl\")\n\nfull.model &lt;- rma.mv(yi = z, \n                     V = var.z, \n                     slab = author,\n                     data = Chernobyl,\n                     random = ~ 1 | author/es.id, \n                     test = \"t\", \n                     method = \"REML\")\n\nsummary(full.model)\n\n\nMultivariate Meta-Analysis Model (k = 33; method: REML)\n\n  logLik  Deviance       AIC       BIC      AICc   \n-21.1229   42.2458   48.2458   52.6430   49.1029   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed        factor \nsigma^2.1  0.1788  0.4229     14     no        author \nsigma^2.2  0.1194  0.3455     33     no  author/es.id \n\nTest for Heterogeneity:\nQ(df = 32) = 4195.8268, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    tval  df    pval   ci.lb   ci.ub      \n  0.5231  0.1341  3.9008  32  0.0005  0.2500  0.7963  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconvert_z2r(0.52)\n\n[1] 0.4777\n\n\nConverting this estimate to \\(r\\), we can see that there is a fairly large correlation of .48, indiciating a substantial relationship between exposure to radiation and mutation rates at Chernobyl."
  },
  {
    "objectID": "posts/post-with-code/index.html#conclusion",
    "href": "posts/post-with-code/index.html#conclusion",
    "title": "Implementing Meta-Regression with R",
    "section": "Conclusion",
    "text": "Conclusion\nMeta-regression offers a bevy of methods and approaches for dealing with dependency and heterogeneity in meta-analysis that would be difficult to deal without clustering or moderators. In this guide I hope I laid out helpful packages for researchers to use and implement in meta-regression in the future."
  }
]